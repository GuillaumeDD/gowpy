{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task on R8 corpus with Frequent Subgraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this Notebook\n",
    "\n",
    "The goal of this notebook is to demonstrate the usage of frequent subgraphs for a multi-class classification task  implemented in the [gowpy library](https://github.com/GuillaumeDD/gowpy) and applied on the R8 dataset. \n",
    "\n",
    "Frequent subgraphs corresponding to long range n-gram can be mined and subsequently used for document classification [1]. Classification with frequent subgraphs happens in a 3-step process:\n",
    "\n",
    "1. Conversion of the corpus of already preprocessed documents into a collection of graph-of-words\n",
    "1. Mining the frequent subgraphs\n",
    "1. Loading the frequent subgraphs and exploiting them for classification\n",
    "\n",
    "\n",
    "Long story short: \n",
    "- the usage of frequent subgraphs shows interesting performance on this dataset ; \n",
    "- however, performance results are a bit under what is obtained via a standard TF-IDF model w.r.t.  accuracy, F1 (macro, micro) and MCC metrics ; and \n",
    "- usage of frequent subgraphs is computation-intensive.\n",
    "\n",
    "The R8 dataset is the preprocessed Reuters dataset with the top 8 classes. It contains 5,495 training documents and 2,189 testing documents, with 8 different labels. Preprocessing involves: tokenization, stop-words removal and stemming to the initial texts. The version of the dataset comes from this [github repository](https://github.com/Nath-B/Graph-Of-Words).\n",
    "\n",
    "[1] [Text Categorization as a Graph Classification Problem](http://www.aclweb.org/anthology/P15-1164).\n",
    "      *Rousseau, François, Kiagias, Emmanouil and Vazirgiannis, Michalis*.\n",
    "      *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International \n",
    "      Joint Conference on Natural Language Processing* (**ACL 2015**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of the python environment:\n",
    "```bash\n",
    "pip install gowpy pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gowpy.feature_extraction.gow import GoWVectorizer\n",
    "from gowpy.gow.miner import GoWMiner\n",
    "from gowpy.gow.io import gow_to_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('champion product approv stock split champion product inc board director approv two for stock split common share for sharehold record april compani board vote recommend sharehold annual meet april increas author capit stock mln mln share reuter',\n",
       " 'earn')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('datasets/r8/r8-train-stemmed.txt',\n",
    "                        header = None, \n",
    "                        sep='\\t', \n",
    "                        names = ['label', 'document'])\n",
    "X_train = df_train['document']\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "earn        2840\n",
       "acq         1596\n",
       "crude        253\n",
       "trade        251\n",
       "money-fx     206\n",
       "interest     190\n",
       "ship         108\n",
       "grain         41\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "earn        1083\n",
       "acq          696\n",
       "crude        121\n",
       "money-fx      87\n",
       "interest      81\n",
       "trade         75\n",
       "ship          36\n",
       "grain         10\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('datasets/r8/r8-test-stemmed.txt',\n",
    "                        header = None, \n",
    "                        sep='\\t', \n",
    "                        names = ['label', 'document'])\n",
    "X_test = df_test['document']\n",
    "y_test = df_test['label']\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Subgraphs Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent subgraphs mining is perform on a \"per label\" basis.\n",
    "\n",
    "From [1] :\n",
    "> In standard  binary graph  classification (e.  g.,  pre-dicting chemical\n",
    "compounds’ carcinogenicity aseither positive  or negative (Helma et al.,\n",
    "2001)), feature mining  is performed on  the whole graph collection  as we\n",
    "expect  the mined  features to  be able to  discriminate between  the two\n",
    "classes (thusproducing a good classifier).  However, for the task of text\n",
    "categorization,  there are  usually more than  two classes  (e.  g.,  118\n",
    "categories of  news articles  for the  Reuters-21578 dataset)  and with\n",
    "a skewed  class   distribution  (e.  g.,   a  lot  more   news related  to\n",
    "“acquisition”  than to  “grain”).   Therefore, a  single support  value\n",
    "might lead  to some classes generating  a tremendous number  of features\n",
    "(e.g., hundreds  of thousands of frequent subgraphs)  and some others\n",
    "only  a few  (e.  g., a  few hundreds subgraphs)  resulting  in a  skewed\n",
    "and non-discriminative feature set.   To include discriminative features\n",
    "for  these minority classes,   we would  need  an  extremely   low  support\n",
    "resulting in    an    exponential     number    of    features    because\n",
    "of the majority classes.  For  these reasons, we decided  to mine frequent\n",
    "subgraphs  per  class  using the  same  relative  support  (%)  and  then\n",
    "aggregating  each   feature  set  into   a  global  one  at   the  cost\n",
    "of a supervised process (but  which still avoids  cross-validated parameter\n",
    "tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the multi-class corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label \"earn\": 2840 documents\n",
      "Label \"acq\": 1596 documents\n",
      "Label \"trade\": 251 documents\n",
      "Label \"ship\": 108 documents\n",
      "Label \"grain\": 41 documents\n",
      "Label \"crude\": 253 documents\n",
      "Label \"interest\": 190 documents\n",
      "Label \"money-fx\": 206 documents\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_to_documents = defaultdict(list)\n",
    "\n",
    "for label, document in zip(y_train, X_train):\n",
    "    label_to_documents[label].append(document)\n",
    "    \n",
    "label_to_documents = dict(label_to_documents)\n",
    "\n",
    "for label, documents in label_to_documents.items():\n",
    "    print(f'Label \"{label}\": {len(list(documents))} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus to gow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gow_miner = GoWMiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_gows = {}\n",
    "\n",
    "for label, documents in label_to_documents.items():\n",
    "    gows = gow_miner.compute_gow_from_corpus(documents)\n",
    "    label_to_gows[label] = gows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph-of-word miner:\n",
       "        - is_directed: False\n",
       "        - is_weighted: False\n",
       "        - window_size: 4\n",
       "        - edge_labeling: True\n",
       "\n",
       "        - Number of tokens: 14575\n",
       "        - Number of links between tokens: 376354\n",
       "\n",
       "        - Number of loaded subgraph: 0\n",
       "        "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gow_miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export gow du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-earn.data --output gbolt-mining-train-r8-earn --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-acq.data --output gbolt-mining-train-r8-acq --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-trade.data --output gbolt-mining-train-r8-trade --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-ship.data --output gbolt-mining-train-r8-ship --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-grain.data --output gbolt-mining-train-r8-grain --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-crude.data --output gbolt-mining-train-r8-crude --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-interest.data --output gbolt-mining-train-r8-interest --dfs --nodes --support 0.096\n",
      "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-money-fx.data --output gbolt-mining-train-r8-money-fx --dfs --nodes --support 0.096\n"
     ]
    }
   ],
   "source": [
    "SUPPORT = \"0.096\"\n",
    "\n",
    "filename_prefixes_output = []\n",
    "\n",
    "for label, gows in label_to_gows.items():\n",
    "    gows = label_to_gows[label]\n",
    "    data = gow_to_data(gows)\n",
    "    \n",
    "    filename_input = f\"train-r8-gows-{label}.data\"\n",
    "    filename_prefix_output = f\"gbolt-mining-train-r8-{label}\"\n",
    "    # Saving the filenames for the loading step of the subgraphs\n",
    "    filename_prefixes_output.append(filename_prefix_output)\n",
    "    \n",
    "    with open(filename_input, \"w\") as f_output:\n",
    "        f_output.write(data)\n",
    "        \n",
    "    print(f\"OMP_NUM_THREADS=1 gbolt --input {filename_input} --output {filename_prefix_output} --dfs --nodes --support {SUPPORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining can be performed via the `gBolt` software (C++) : https://github.com/Jokeren/gBolt :\n",
    "\n",
    "```sh\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-earn.data --output gbolt-mining-train-r8-earn --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-acq.data --output gbolt-mining-train-r8-acq --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-trade.data --output gbolt-mining-train-r8-trade --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-ship.data --output gbolt-mining-train-r8-ship --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-grain.data --output gbolt-mining-train-r8-grain --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-crude.data --output gbolt-mining-train-r8-crude --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-interest.data --output gbolt-mining-train-r8-interest --dfs --nodes --support 0.096\n",
    "OMP_NUM_THREADS=1 gbolt --input train-r8-gows-money-fx.data --output gbolt-mining-train-r8-money-fx --dfs --nodes --support 0.096\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Results and Some Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'gbolt-mining-train-r8-earn.t0' and 'gbolt-mining-train-r8-earn.nodes'\n",
      "Loading 'gbolt-mining-train-r8-acq.t0' and 'gbolt-mining-train-r8-acq.nodes'\n",
      "Loading 'gbolt-mining-train-r8-trade.t0' and 'gbolt-mining-train-r8-trade.nodes'\n",
      "Loading 'gbolt-mining-train-r8-ship.t0' and 'gbolt-mining-train-r8-ship.nodes'\n",
      "Loading 'gbolt-mining-train-r8-grain.t0' and 'gbolt-mining-train-r8-grain.nodes'\n",
      "Loading 'gbolt-mining-train-r8-crude.t0' and 'gbolt-mining-train-r8-crude.nodes'\n",
      "Loading 'gbolt-mining-train-r8-interest.t0' and 'gbolt-mining-train-r8-interest.nodes'\n",
      "Loading 'gbolt-mining-train-r8-money-fx.t0' and 'gbolt-mining-train-r8-money-fx.nodes'\n"
     ]
    }
   ],
   "source": [
    "for filename_prefix in filename_prefixes_output:\n",
    "    filename_t0 = f'{filename_prefix}.t0'\n",
    "    filename_nodes = f'{filename_prefix}.nodes'\n",
    "    print(f\"Loading '{filename_t0}' and '{filename_nodes}'\")\n",
    "    gow_miner.load_graphs(filename_t0, filename_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph-of-word miner:\n",
       "        - is_directed: False\n",
       "        - is_weighted: False\n",
       "        - window_size: 4\n",
       "        - edge_labeling: True\n",
       "\n",
       "        - Number of tokens: 14575\n",
       "        - Number of links between tokens: 376354\n",
       "\n",
       "        - Number of loaded subgraph: 31682\n",
       "        "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gow_miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31682.000000\n",
       "mean         0.008834\n",
       "std          0.017117\n",
       "min          0.000729\n",
       "25%          0.003464\n",
       "50%          0.003646\n",
       "75%          0.004558\n",
       "max          0.487694\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_freq_per_pattern = pd.Series(gow_miner.stat_relative_freq_per_pattern())\n",
    "s_freq_per_pattern.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa1a1670f10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_freq_per_pattern.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31682.000000\n",
       "mean         6.110031\n",
       "std          1.452774\n",
       "min          1.000000\n",
       "25%          5.000000\n",
       "50%          6.000000\n",
       "75%          7.000000\n",
       "max          9.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_num_nodes_per_pattern = pd.Series(gow_miner.stat_num_nodes_per_pattern())\n",
    "s_num_nodes_per_pattern.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa1a16b8750>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_num_nodes_per_pattern.plot.box(vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoW Vectorizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_gow = GoWVectorizer(gow_miner, indexing=True, min_df=0.0, max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer_gow.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5485x31682 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1641028 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput termin system cpml complet sale comput termin system inc complet sale share common stock and warrant acquir addit mln share sedio lugano switzerland for dlr compani warrant exercis for year purchas price dlr per share comput termin sedio bui addit share and increas total hold pct comput termin outstand common stock circumst involv chang control compani compani condit occur warrant exercis price equal pct common stock market price time not exce dlr per share comput termin sold technolgi right dot matrix impact technolog includ futur improv woodco inc houston tex for dlr continu exclus worldwid license technolog for woodco compani move part reorgan plan and pai current oper cost and ensur product deliveri comput termin make comput gener label form tag and ticket printer and termin reuter\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer_gow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features: 61\n",
      "compani\n",
      "comput\n",
      "product and product__and\n",
      "share mln share__mln\n",
      "bui\n",
      "purchas\n",
      "per\n",
      "pct\n",
      "common stock stock__common\n",
      "move\n",
      "cost\n",
      "futur\n",
      "sale\n",
      "acquir\n",
      "dlr\n",
      "for\n",
      "gener\n",
      "for year for__year\n",
      "mln\n",
      "control\n",
      "year dlr year__dlr\n",
      "share\n",
      "for dlr for__dlr\n",
      "plan\n",
      "outstand\n",
      "hold\n",
      "share per share__per\n",
      "common\n",
      "product\n",
      "current\n",
      "part\n",
      "stock\n",
      "reuter and reuter__and\n",
      "increas and increas__and\n",
      "market\n",
      "pct total pct__total\n",
      "price dlr dlr__price\n",
      "make\n",
      "not\n",
      "share and share__and\n",
      "oper\n",
      "and cost and__cost\n",
      "and\n",
      "common share common__share\n",
      "for compani for__compani\n",
      "for price for__price\n",
      "inc\n",
      "reuter\n",
      "pai\n",
      "includ\n",
      "chang\n",
      "increas\n",
      "year\n",
      "time\n",
      "share dlr share__dlr\n",
      "price\n",
      "continu\n",
      "total\n",
      "for share dlr share__dlr for__dlr\n",
      "complet\n",
      "system\n"
     ]
    }
   ],
   "source": [
    "features = [feature for presence, feature in zip(X.toarray()[1], feature_names) if presence > 0]\n",
    "print(\"Nombre de features: {}\".format(len(features)))\n",
    "for feature in features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task with Frequent Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-optimisation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
    "scorer_mcc = make_scorer(matthews_corrcoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/!\\** Hyperparameter search is a time-consuming and computation-intensive procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gowpy.feature_extraction.gow.gow_vectorizer import SUBGRAPH_MATCHING_INDUCED, SUBGRAPH_MATCHING_PARTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print(\"## GoW Model ##\")\n",
    "pipeline = Pipeline([\n",
    "    ('gow', GoWVectorizer(gow_miner)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', LinearSVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'gow__subgraph_matching' : [SUBGRAPH_MATCHING_INDUCED],\n",
    "    'gow__indexing' : [True],\n",
    "#\n",
    "# min          0.000729\n",
    "# 25%          0.003464\n",
    "# 50%          0.003646\n",
    "# 75%          0.004558\n",
    "# max          0.487694\n",
    "    'gow__min_df' : [0.0, 0.003464],\n",
    "    'gow__max_df' : [0.5],\n",
    "#\n",
    "    'svm__C' : [0.1, 1, 10, 100],\n",
    "    'svm__class_weight' : ['balanced'],\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           parameters, \n",
    "                           cv=10,\n",
    "                           scoring=scorer_mcc,\n",
    "                           n_jobs=-1, verbose=10)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 GoWVectorizer(graph_of_words=Graph-of-word miner:\n",
       "        - is_directed: False\n",
       "        - is_weighted: False\n",
       "        - window_size: 4\n",
       "        - edge_labeling: True\n",
       "\n",
       "        - Number of tokens: 14575\n",
       "        - Number of links between tokens: 376354\n",
       "\n",
       "        - Number of loaded subgraph: 31682\n",
       "        ,\n",
       "                               subgraph_matching='induced')),\n",
       "                ('tfid', TfidfTransformer()),\n",
       "                ('svm', LinearSVC(C=1, class_weight='balanced'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_C = 1\n",
    "param_class_weight = 'balanced'\n",
    "\n",
    "pipeline_gow = Pipeline([\n",
    "    ('vect', GoWVectorizer(gow_miner, \n",
    "                           min_df=0.0,\n",
    "                           max_df=1.0,\n",
    "                           subgraph_matching='induced',\n",
    "                           indexing=True\n",
    "                          )),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('svm', LinearSVC(\n",
    "        C=param_C,\n",
    "        class_weight=param_class_weight,\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipeline_gow.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acq       0.97      0.97      0.97       696\n",
      "       crude       0.94      0.95      0.95       121\n",
      "        earn       0.99      0.99      0.99      1083\n",
      "       grain       1.00      1.00      1.00        10\n",
      "    interest       0.86      0.78      0.82        81\n",
      "    money-fx       0.80      0.83      0.81        87\n",
      "        ship       0.85      0.92      0.88        36\n",
      "       trade       0.92      0.97      0.95        75\n",
      "\n",
      "    accuracy                           0.96      2189\n",
      "   macro avg       0.92      0.93      0.92      2189\n",
      "weighted avg       0.96      0.96      0.96      2189\n",
      "\n",
      "mcc=0.9442491118465957 ; accuracy=0.9639104613978986 ; f1-micro=0.9639104613978986\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print('Results on the test set:')\n",
    "\n",
    "y_pred = pipeline_gow.predict(X_test)\n",
    "y_true = y_test\n",
    "    \n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='micro')\n",
    "print(f\"mcc={mcc} ; accuracy={accuracy} ; f1-micro={f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gowpy-classification]",
   "language": "python",
   "name": "conda-env-gowpy-classification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
